### LLaMA Factory — Joker LoRA 微调配置
### 用法: llamafactory-cli train llamafactory_config/joker_lora.yaml

### 模型
model_name_or_path: deepseek-ai/DeepSeek-R1-Distill-Qwen-14B
# 备选: Qwen/Qwen2.5-14B-Instruct（如果 R1 效果不好可以换回）

### 训练方法
stage: sft
do_train: true
finetuning_type: lora
lora_rank: 64
lora_alpha: 128
lora_dropout: 0.05
lora_target: all
# all = 对所有线性层做 LoRA，效果最好

### 数据
dataset: joker_sft
template: qwen
cutoff_len: 1024
overwrite_cache: true
preprocessing_num_workers: 8

### 训练超参
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
# 有效 batch size = 2 × 8 = 16
learning_rate: 2.0e-5
num_train_epochs: 5
lr_scheduler_type: cosine
warmup_ratio: 0.1
weight_decay: 0.01
max_grad_norm: 1.0

### 精度
bf16: true
# A100 支持 bf16，不需要量化

### 输出
output_dir: ./output/joker-lora-r1-14b
logging_steps: 5
save_steps: 100
save_total_limit: 3

### 其他
seed: 42
report_to: none
